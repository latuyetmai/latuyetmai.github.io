{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Pipeline: UNDERSTANDING USER BEHAVIOR\n",
    "-----------\n",
    "\n",
    "\n",
    "# A. Description of Data Pipeline:\n",
    "\n",
    "## I. Instrument API server and send log events to Kafka\n",
    "\n",
    "### 1. Spin up the pipeline\n",
    "\n",
    "Steps to spin up the pipeline: start with setting up the enviroment using docker-compose.yml\n",
    "\n",
    "* Navigate to the project folder where the docker-compose.yml file is stored.\n",
    "* Explanation about the docker-compose.yml file: \n",
    "It includes the following containers\n",
    "    - zookeeper: Docker image for running Zookeeper, set expose to port 32181\n",
    "    - kafka: Docker image for running Kafka, have single broker cluster with broker id = 1, set depends on zookeeper and connect to zookeeper through port 32181, expose to port 29092\n",
    "    - cloudera: Docker image for running Hadoop, version 0.0.2, expose to port 8888 for hue and 9083 for hive thrift server, allow other program to connect to cloudera through port 8888. \n",
    "    - spark: Docker image for running Spark with Python, set depends on cloudera, connect to Hadoop environment using namenode cloudera, and connect to Hive thrift server cloudera through port 9083.\n",
    "    - presto: Docker image for running Presto, connect to Hive thrift server cloudera through port 9083\n",
    "    - mids: Docker image for running Flask, Apache Bench, Python with datascience libraries, etc.\n",
    "\n",
    "\n",
    "* Check what container exist before spinning up the cluster:\n",
    "\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "\n",
    "* Spin up multiple Docker containers from the docker-compose.yml file, running the containers in the background:\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "* Check what containers are running after spinning up the cluster, show container names and port details:\n",
    "\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "* Look at the logs to check Kafka broker:  \n",
    "\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "\n",
    "```\n",
    "\n",
    "* Check (list) what files/ folders are currently in the /tmp/ directory in Hadoop prior to streaming data, we will not see the streaming data directories in this step yet:\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "### 2. Running API Server/ Web-app with Flask and send the event logs to Kafka\n",
    "\n",
    "Running an API Server using Flask and sending the event logs to Kafka, which is executed by running the python scripts game_api.py. \n",
    "\n",
    "* This python script contain codes with the following functions:\n",
    "    - Flask retrieves \"GET\" and \"POST\" requests for different event types. The following events are used in this project:\n",
    "        * defaul_reponse: \"GET\" request \n",
    "        * purchase_a_sword: \"GET\" request\n",
    "        * join_a_guild:  \"GET\" request\n",
    "        * Earn_more_gold:  \"GET\" request\n",
    "        * purchase_a_potion: \"GET\" and \"POST\" request\n",
    "    - The API server is acting as a producer sends these event logs to Kafka bootstrap_server through port 29002 under topic name `events`. Each message is an event log from a \"GET\" or \"POST\" request event.\n",
    "\n",
    "* Command for running flask from the scripts while streaming:\n",
    "\n",
    "```\n",
    "docker-compose exec mids \\\n",
    "  env FLASK_APP=/Assemble_Pipeline_API_Streaming/game_api.py \\\n",
    "  flask run --host 0.0.0.0\n",
    "```\n",
    "\n",
    "### 3. Use Kafkacat consumer to watch for kafka messages:\n",
    "\n",
    "Open Terminal 2 window for setting up to watch the messages sent through Kafka:\n",
    "\n",
    "* Create a Kafka topic name `events` (eventhough this step is not required, setting up the topic prior to watching Kafka could avoid seeing the topic event error when running the code for watching). This topic has 1 partition and connect to zookeeper through port 32181\n",
    "\n",
    "```\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic events \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "* Use kafkacat consumer to read the messages sent in topic `events` from the beginning through port 29092. We will see the messages coming in during streaming the data\n",
    "\n",
    "```\n",
    "docker-compose exec mids \\\n",
    "  kafkacat -C -b kafka:29092 -t events -o beginning\n",
    "```\n",
    "\n",
    "### 4. Run Spark from python scripts to consume streaming messages and land on Hadoop for later data streaming \n",
    "\n",
    "Open Terminal 3 window for running Spark\n",
    "\n",
    "* Run the python script write_swords_stream.py to consume the Kafka messages, filter select event types and land them into HDFS parquet file. This python script contain codes with the following functions:\n",
    "\n",
    "    - Build a session for running Spark \n",
    "    - Consume the streaming messages from kafka topic `events` by readStream function, connect to kafka through port 29092. .\n",
    "    - Tranform the messages with Spark and filter/ select the events of interest:\n",
    "        * Specify the event schema\n",
    "        * Filter from raw events and select the event of interested (for example filter the event type \"purchase_sword\", and event type \"join_guild\")\n",
    "        * Cast the timestamp, event value and event schema from binary data to string\n",
    "        * Return the event value, timestamp and event schema in the streaming messages\n",
    "    - Write the streaming data to parquet file with writeStream function and land into Hadoop with processing time every 120 seconds\n",
    "    - Streaming the data until termination (Crtl + C)\n",
    "\n",
    "Codes for running Spark from write_stream.py\n",
    "\n",
    "```\n",
    "docker-compose exec spark \\\n",
    "  spark-submit /Assemble_Pipeline_API_Streaming/write_stream.py\n",
    "```\n",
    "\n",
    "### 5. Check the data landed in Hadoop:\n",
    "\n",
    "Open Terminal 4 window for watching Hadoop\n",
    "\n",
    "* List all directories in hadoop /tmp/ directory\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "* Check sword_purchase directory in hadoop with human readable format\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/sword_purchases\n",
    "```\n",
    "\n",
    "* Check guild_joins directory in hadoop with human readable format\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/guild_joins\n",
    "```\n",
    "\n",
    "* Check gold_earns directory in hadoop with human readable format\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/gold_earns\n",
    "```\n",
    "\n",
    "\n",
    "* Check potion_purchases directory in hadoop with human readable format\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/potion_purchases\n",
    "```\n",
    "\n",
    "* Check all_events directory in hadoop with human readable format\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/all_events\n",
    "```\n",
    "\n",
    "## II. Create Batch Events\n",
    "\n",
    "### 1. Use Apache Bench to generate batch events, test data:\n",
    "\n",
    "Open Terminal 5 window for using Apache Bench to generate the events in a batch, we can see these events in the Kafka watch window, and the Flask window. \n",
    "\n",
    "* Run the bash script file generate_data.sh which includes codes for generating API requests using Apache Bench:\n",
    "\n",
    "```\n",
    "./generate_data.sh\n",
    "```\n",
    "\n",
    "  - Example of a GET request in the bash script generate_data.sh:\n",
    "\n",
    "```\n",
    "docker-compose exec mids \\\n",
    "  ab \\\n",
    "    -n 5 \\\n",
    "    -H \"Host: user1.comcast.com\" \\\n",
    "    http://localhost:5000/purchase_a_sword/titanium\n",
    "```\n",
    "  \n",
    "  - Example of a POST request in the bash script generate_data.sh:\n",
    "\n",
    "```\n",
    "docker-compose exec mids \\\n",
    "  ab \\\n",
    "    -n 3 \\\n",
    "    -H \"Host: user2.att.com\" \\\n",
    "    -T \"application/json\" \\\n",
    "    -p /w205/project-3-latuyetmai/post.txt \\\n",
    "    http://localhost:5000/purchase_a_potion\n",
    "```\n",
    "\n",
    "## III. Query Batch Data with Presto\n",
    "\n",
    "### 1. Create external tables in the hive metastore in hadoop ecosystem\n",
    "\n",
    "Open Terminal 4 window for running hive and then presto\n",
    "\n",
    "* Running hive:\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hive\n",
    "```\n",
    "\n",
    "* Create an external table for the purchase events, named it purchases:\n",
    "\n",
    "```\n",
    "create external table if not exists default.sword_purchases (\n",
    "    event_type string,\n",
    "    gold_count string,\n",
    "    description string,\n",
    "    value string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    timestamp string,\n",
    "    raw_event string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/sword_purchases'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "* Create an external table for the guild join events, named it purchases:\n",
    "\n",
    "```\n",
    "create external table if not exists default.guild_joins (\n",
    "    event_type string,\n",
    "    gold_count string,\n",
    "    description string,\n",
    "    value string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    timestamp string,\n",
    "    raw_event string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/guild_joins'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "* Create an external table for the gold earn events, named it purchases:\n",
    "\n",
    "```\n",
    "create external table if not exists default.gold_earns (\n",
    "    event_type string,\n",
    "    gold_count string,\n",
    "    description string,\n",
    "    value string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    timestamp string,\n",
    "    raw_event string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/gold_earns'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "* Create an external table for the knife purchase events, named it knife_purchases:\n",
    "\n",
    "```\n",
    "create external table if not exists default.potion_purchases (\n",
    "    event_type string,\n",
    "    gold_count string,\n",
    "    description string,\n",
    "    value string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    timestamp string,\n",
    "    raw_event string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/potion_purchases'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "* Create an external table for all events, named it all_events:\n",
    "\n",
    "```\n",
    "create external table if not exists default.all_events (\n",
    "    event_type string,\n",
    "    gold_count string,\n",
    "    description string,\n",
    "    value string,\n",
    "    Accept string,\n",
    "    Host string,\n",
    "    User_Agent string,\n",
    "    timestamp string,\n",
    "    raw_event string\n",
    "  )\n",
    "  stored as parquet \n",
    "  location '/tmp/all_events'\n",
    "  tblproperties (\"parquet.compress\"=\"SNAPPY\");\n",
    "```\n",
    "\n",
    "* Close hive with `Ctrl-D`\n",
    "\n",
    "### 2. Query the table with presto:\n",
    "\n",
    "* Running Presto:\n",
    "\n",
    "    - Presto is connected to hadoop (hive thrift server) through port 9083, and so it could get the external table that was added in hive. Code for running Presto:\n",
    "\n",
    "```\n",
    "docker-compose exec presto presto --server presto:8080 --catalog hive --schema default\n",
    "\n",
    "```\n",
    "\n",
    "* Show the tables that we have in hive with Presto:\n",
    "\n",
    "```\n",
    "show tables;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|      Table       |\n",
    "|:----------------:|\n",
    "|    all_events    |\n",
    "|    gold_earns    |\n",
    "|   guild_joins    |\n",
    "| potion_purchases |\n",
    "| sword_purchases  |\n",
    "\n",
    "* Show the summary of a table with describe function:\n",
    "\n",
    "```\n",
    "describe sword_purchases;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|    Column   |   Type  | Comment |\n",
    "|:-----------:|:-------:|:-------:|\n",
    "|  event_type | varchar |         |\n",
    "|  gold_count | varchar |         |\n",
    "| description | varchar |         |\n",
    "|    value    | varchar |         |\n",
    "|    accept   | varchar |         |\n",
    "|     host    | varchar |         |\n",
    "|  user_agent | varchar |         |\n",
    "|  timestamp  | varchar |         |\n",
    "|  raw_event  | varchar |         |\n",
    "\n",
    "* Query: show the first 10 rows data of each table:\n",
    "\n",
    "```\n",
    "select * from all_events limit 10;\n",
    "select * from gold_earns limit 10;\n",
    "select * from guild_joins limit 10;\n",
    "select * from potion_purchases limit 10;\n",
    "select * from sword_purchases limit 10;\n",
    "\n",
    "```\n",
    "    Note: exit out of the table with `q`\n",
    "\n",
    "* Query: How many total events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from all_events;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        132      |\n",
    "\n",
    "\n",
    "* Query: How many gold_earns events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from gold_earns;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        70       |\n",
    "\n",
    "* Query: How many guild_joins events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from guild_joins;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        7        |\n",
    "\n",
    "\n",
    "* Query: How many potion_purchases events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from potion_purchases;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        14       |\n",
    "\n",
    "\n",
    "* Query: How many purchase_sword events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from sword_purchases;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        21       |\n",
    "\n",
    "\n",
    "# B. Streaming & Analyse Data:\n",
    "\n",
    "\n",
    "### 1. Sending streaming data continuously with Apache Bench:\n",
    "\n",
    "Open Terminal 5 for sending streaming data\n",
    "\n",
    "* Sending API requests every 30 second, for example sending all the requests from the generate_data.sh file until termination (Ctrl + C):\n",
    "\n",
    "```\n",
    "while true; do\n",
    "    ./generate_data.sh\n",
    "    sleep 30\n",
    "done\n",
    "```\n",
    "\n",
    "* See streaming events in the terminal running Flask, Kafka and Spark. \n",
    "\n",
    "### 2. Analyse streaming data with presto:\n",
    "\n",
    "Update the queries in Terminal 4 running presto while streaming, watch and see the data growing. Perform query after some time.\n",
    "\n",
    "* Query: How many total sword_purchase events?\n",
    "\n",
    "```\n",
    "select count(*) as event_count from sword_purchases;\n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|   event_count   |\n",
    "|:---------------:|\n",
    "|        210      |\n",
    "\n",
    "* Query: How many events each user made, what type of sword, potion they purchased and guild name they join?\n",
    "\n",
    "```\n",
    "select host as player, event_type, description, count(raw_event) as number_of_event from all_events group by host, event_type, description order by host, event_type, number_of_event desc;\n",
    "    \n",
    "```\n",
    "\n",
    "    Result:\n",
    "\n",
    "|       player      |    event_type   |       description      | number_of_event |\n",
    "|:-----------------:|:---------------:|:----------------------:|:---------------:|\n",
    "| user1.comcast.com |     default     |                        |       120       |\n",
    "| user1.comcast.com |    earn_gold    |                        |       420       |\n",
    "| user1.comcast.com |    join_guild   |       holy_ramen       |        24       |\n",
    "| user1.comcast.com |    join_guild   |     kungfu_chicken     |        24       |\n",
    "| user1.comcast.com | purchase_potion |        strength        |        36       |\n",
    "| user1.comcast.com | purchase_potion |         poison         |        13       |\n",
    "| user1.comcast.com | purchase_potion |           fly          |        12       |\n",
    "| user1.comcast.com | purchase_potion | Not_enough_gold_to_buy |        11       |\n",
    "| user1.comcast.com |  purchase_sword | Not_enough_gold_to_buy |       118       |\n",
    "| user1.comcast.com |  purchase_sword |        titanium        |        49       |\n",
    "| user1.comcast.com |  purchase_sword |          steel         |        13       |\n",
    "|   user2.att.com   |     default     |                        |       120       |\n",
    "|   user2.att.com   |    earn_gold    |                        |       420       |\n",
    "|   user2.att.com   |    join_guild   |       holy_ramen       |        12       |\n",
    "|   user2.att.com   |    join_guild   |     kungfu_chicken     |        12       |\n",
    "|   user2.att.com   |    join_guild   |      clumsy_witch      |        12       |\n",
    "|   user2.att.com   | purchase_potion |        strength        |        36       |\n",
    "|   user2.att.com   | purchase_potion | Not_enough_gold_to_buy |        35       |\n",
    "|   user2.att.com   | purchase_potion |          speed         |        24       |\n",
    "|   user2.att.com   | purchase_potion |          love          |        1        |\n",
    "|   user2.att.com   |  purchase_sword | Not_enough_gold_to_buy |        66       |\n",
    "|   user2.att.com   |  purchase_sword |         silver         |        5        |\n",
    "|   user2.att.com   |  purchase_sword |          magic         |        1        |\n",
    " \n",
    "\n",
    "* Query: How many guilds are there and how many member in each guild?\n",
    "\n",
    "```\n",
    "select description as guid_name, max(value) as number_of_members from guild_joins group by description order by number_of_members desc;  \n",
    "``` \n",
    "    Result:\n",
    "\n",
    "|    guid_name   |   number_of_members  |\n",
    "|:--------------:|:--------------------:|\n",
    "|   holy_ramen   |          3          |\n",
    "| kungfu_chicken |          2          |\n",
    "|  clumsy_witch  |          1          |\n",
    "\n",
    "* Query: How many sword did each player purchase by sword type and value?\n",
    "\n",
    "```\n",
    "select host as player, description as sword_type, value as sword_price, count(raw_event) as number_of_purchases  from sword_purchases group by host, description, value order by player, number_of_purchases desc;  \n",
    "```\n",
    "\n",
    "    Result:\n",
    "    \n",
    "|       player      |       sword_type       | sword_price | number_of_purchases |\n",
    "|:-----------------:|:----------------------:|:-----------:|:-------------------:|\n",
    "| user1.comcast.com | Not_enough_gold_to_buy |      0      |         118         |\n",
    "| user1.comcast.com |        titanium        |      4      |          49         |\n",
    "| user1.comcast.com |          steel         |      1      |          13         |\n",
    "|   user2.att.com   | Not_enough_gold_to_buy |      0      |          66         |\n",
    "|   user2.att.com   |         silver         |      2      |          5          |\n",
    "|   user2.att.com   |          magic         |      20     |          1          |\n",
    "\n",
    "* Query: How many potion did each player purchase by potion type and value?\n",
    "\n",
    "```\n",
    "select host as player, description as potion_type, value as potion_price, count(raw_event) as number_of_purchases  from potion_purchases group by host, description, value order by player, number_of_purchases desc;  \n",
    "```\n",
    "\n",
    "    Result:\n",
    "    \n",
    "|       player      |       potion_type      | potion_price | number_of_purchases |\n",
    "|:-----------------:|:----------------------:|:------------:|:-------------------:|\n",
    "| user1.comcast.com |        strength        |       3      |          36         |\n",
    "| user1.comcast.com |         poison         |       4      |          13         |\n",
    "| user1.comcast.com |           fly          |       5      |          12         |\n",
    "| user1.comcast.com | Not_enough_gold_to_buy |       0      |          11         |\n",
    "|   user2.att.com   |        strength        |       3      |          36         |\n",
    "|   user2.att.com   | Not_enough_gold_to_buy |       0      |          35         |\n",
    "|   user2.att.com   |          speed         |       2      |          24         |\n",
    "|   user2.att.com   |          love          |       6      |          1          |\n",
    "\n",
    "### 3. Spin down Docker containers\n",
    "\n",
    "* Terminal 5: Stop sending streaming data with `Ctrl-C`\n",
    "* Terminal 4: Stop running presto with `Ctrl-D`\n",
    "* Terminal 3: Stop running Spark with `Ctrl-C`\n",
    "* Terminal 2: Stop running Kafka with `Ctrl-C`\n",
    "* Terminal 1: Stop running Flask with `Ctrl-C`\n",
    "* Spin down docker containers from the docker-compose.yml file:\n",
    "\n",
    "```\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "* Check if the containers are down:\n",
    "\n",
    "```\n",
    "docker-compose ps\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
