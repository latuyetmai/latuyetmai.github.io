{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Data Pipeline: Tracking User Activity\n",
    "-------\n",
    "\n",
    "In this project, I will use a nested json data extracted from an ed tech firm database, who provides service that delivers various assessments to different customers in Tech. I will explain the detailed steps on how to extract, transform and load (ETL) the data through the pipeline and prepare the data ready for data scientists to run queries on.\n",
    "\n",
    "The main goal of this project is explaining the pipeline as demonstrated in the following steps.\n",
    "\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Prepare the infrastructure to land the data in the form and structure it needs to be in order to run queries. I will perform the following tasks:\n",
    "\n",
    "- Publish and consume messages with Kafka, the message sent is the nested json data.\n",
    "- Use Spark to transform the messages so that they can be landed in Hadoop (HDFS)\n",
    "- Run queries, brief analysis of the transformed data  \n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "To get the data, run in the CLI\n",
    "```\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "```\n",
    "\n",
    "Note on the data: This dataset is a nested JSON file, where it will need to be unwraped carefully to understand what's really being displayed.\n",
    "\n",
    "\n",
    "## I. Publish & Consume massages with Kafka\n",
    "\n",
    "### 1. Spin up the pipeline\n",
    "\n",
    "Steps to spin up the pipeline: start with setting up the enviroment using docker-compose\n",
    "\n",
    "* Navigate to the project folder where the docker-compose.yml file is stored\n",
    "* Explanation about the docker-compose.yml file: It includes the following containers\n",
    "    - zookeeper: Docker image for running Zookeeper, set expose to port 32181\n",
    "    - kafka: Docker image for running Kafka, have single broker cluster with broker id = 1, set depends on zookeeper and connect to zookeeper through port 32181, expose to port 29092\n",
    "    - cloudera: Docker image for running Hadoop\n",
    "    - spark: Docker image for running Spark with Python, set depends on cloudera connect with Hadoop using namenode cloudera, expose to port 8888 for running Jupyter notebook\n",
    "    - mids: Docker image for running Linux, jq, Python with datascience libraries\n",
    "\n",
    "\n",
    "* Check what container exist before spinning up the cluster:\n",
    "\n",
    "```\n",
    "docker ps\n",
    "```\n",
    "\n",
    "* Spin up multiple Docker containers from the docker-compose.yml file, running the containers in the background:\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "```\n",
    "\n",
    "* Check what containers are running after spinning up the cluster:\n",
    "\n",
    "```\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "* Look at the logs to check Kafka broker:  \n",
    "\n",
    "```\n",
    "docker-compose logs -f kafka\n",
    "\n",
    "```\n",
    "\n",
    "* Check (list) what files/ folders are currently in the /tmp/ directory in Hadoop:\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "### 2. Initial data exploratory with jq to understand the context\n",
    "\n",
    "* Look through the structure of the data:\n",
    "    \n",
    "    - Code summary: Using jq pretty print display the first 200 lines of the data\n",
    "\n",
    "```\n",
    "cat assessment-attempts-20180128-121051-nested.json | jq . | head -n200\n",
    "```\n",
    "\n",
    "* Explore question: How many assessments in the data set?\n",
    "\n",
    "    - Code summary: Print each json object in each line, and count the number of lines \n",
    "    \n",
    "```\n",
    "cat assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | wc -l\n",
    "```\n",
    "\n",
    "* Explore question: What are the most common courses?\n",
    "\n",
    "    - Code summary: Print the value of key `exam_name` in each line, get the unique values for exam_name, sort them from bigger to smaller and display the first 10 values.\n",
    "\n",
    "```\n",
    "cat assessment-attempts-20180128-121051-nested.json | jq '.[]|.exam_name' -c | sort | uniq -c | sort -gr |  head -10\n",
    "```\n",
    "\n",
    "### 3. Create a Kafka topic & publish messages\n",
    "\n",
    "* Decide Kafka topic name: \n",
    "    - the data exploratory using jq show the data contain records of different exams that users took. Each exam recorded the exam name, exam id, user id, when they took the exam, and details of the exam result (each question's result, total of questions, and correct answers, etc.)\n",
    "    - Therefore, I name the Kafka topic for this project `assessments` to reflect the summary of the data\n",
    "    \n",
    "    \n",
    "* Create a Kafka topic:\n",
    "\n",
    "    - Code summary: Running kafka. Create a Kafka topic name `assessments` with 1 partition, set replication factor to 1, and connect with zookeeper through port 32181\n",
    "\n",
    "```\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --create \\\n",
    "    --topic assessments \\\n",
    "    --partitions 1 \\\n",
    "    --replication-factor 1 \\\n",
    "    --if-not-exists \\\n",
    "--zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "* Check the topic created:\n",
    "\n",
    "    - Code summary: Running kafka, display an overview of topic `assessments`\n",
    "\n",
    "```\n",
    "docker-compose exec kafka \\\n",
    "  kafka-topics \\\n",
    "    --describe \\\n",
    "    --topic assessments \\\n",
    "--zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "* Publish messages using the assessment data .json file:\n",
    "\n",
    "    - Code summary: Using kafkacat, the producer (`P`) publishes messages to the kafka topic `assessments` through port 29092, each message is created from printting out a line object in the json data.\n",
    "\n",
    "```\n",
    "docker-compose exec mids \\\n",
    "  bash -c \"cat /w205/project-2-latuyetmai/assessment-attempts-20180128-121051-nested.json \\\n",
    "    | jq '.[]' -c \\\n",
    "    | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "\n",
    "\n",
    "### 4. Spin up Spark pyspark and open Jupyter notebook \n",
    "\n",
    "   - Code summary: Running spark pyspark (spark with python), using jupyter notebook connect on port 8888\n",
    "\n",
    "```\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root --notebook-dir=/ETL_Pipeline_Tracking_User_Activities' pyspark\n",
    "```\n",
    "\n",
    "## II. Use Spark to transform the messages\n",
    "\n",
    "### 1. Consume the messages from Kafka\n",
    "\n",
    "   - Code summary: Using spark consuming the messages from kafka topic `assessments`, connect to kafka through port 29092. Read the messages from earliest to latest offset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consume the messages from kafka topic `assessments`\n",
    "raw_data = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the data schema\n",
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     0|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     1|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     2|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     3|1969-12-31 23:59:...|            0|\n",
      "|null|[7B 22 6B 65 65 6...|assessments|        0|     4|1969-12-31 23:59:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See raw messages\n",
    "raw_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform the messages with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache() method save the dataset to storage level \n",
    "raw_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "|{\"keen_timestamp\"...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cast binary data as string\n",
    "raw_assessments = raw_data.select(raw_data.value.cast('string'))\n",
    "raw_assessments.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'{\"keen_timestamp\":\"1516717442.735266\",\"max_attempts\":\"1.0\",\"started_at\":\"2018-01-23T14:23:19.082Z\",\"base_exam_id\":\"37f0a30a-7464-11e6-aa92-a8667f27e5dc\",\"user_exam_id\":\"6d4089e4-bde5-4a22-b65f-18bce9ab79c8\",\"sequences\":{\"questions\":[{\"user_incomplete\":true,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:24.670Z\",\"id\":\"49c574b4-5c82-4ffd-9bd1-c3358faf850d\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:25.914Z\",\"id\":\"f2528210-35c3-4320-acf3-9056567ea19f\",\"submitted\":1,\"correct\":true},{\"checked\":false,\"correct\":true,\"id\":\"d1bf026f-554f-4543-bdd2-54dcf105b826\"}],\"user_submitted\":true,\"id\":\"7a2ed6d3-f492-49b3-b8aa-d080a8aad986\",\"user_result\":\"missed_some\"},{\"user_incomplete\":false,\"user_correct\":false,\"options\":[{\"checked\":true,\"at\":\"2018-01-23T14:23:30.116Z\",\"id\":\"a35d0e80-8c49-415d-b8cb-c21a02627e2b\",\"submitted\":1},{\"checked\":false,\"correct\":true,\"id\":\"bccd6e2e-2cef-4c72-8bfa-317db0ac48bb\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:41.791Z\",\"id\":\"7e0b639a-2ef8-4604-b7eb-5018bd81a91b\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"bbed4358-999d-4462-9596-bad5173a6ecb\",\"user_result\":\"incorrect\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"at\":\"2018-01-23T14:23:52.510Z\",\"id\":\"a9333679-de9d-41ff-bb3d-b239d6b95732\"},{\"checked\":false,\"id\":\"85795acc-b4b1-4510-bd6e-41648a3553c9\"},{\"checked\":true,\"at\":\"2018-01-23T14:23:54.223Z\",\"id\":\"c185ecdb-48fb-4edb-ae4e-0204ac7a0909\",\"submitted\":1,\"correct\":true},{\"checked\":true,\"at\":\"2018-01-23T14:23:53.862Z\",\"id\":\"77a66c83-d001-45cd-9a5a-6bba8eb7389e\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"e6ad8644-96b1-4617-b37b-a263dded202c\",\"user_result\":\"correct\"},{\"user_incomplete\":false,\"user_correct\":true,\"options\":[{\"checked\":false,\"id\":\"59b9fc4b-f239-4850-b1f9-912d1fd3ca13\"},{\"checked\":false,\"id\":\"2c29e8e8-d4a8-406e-9cdf-de28ec5890fe\"},{\"checked\":false,\"id\":\"62feee6e-9b76-4123-bd9e-c0b35126b1f1\"},{\"checked\":true,\"at\":\"2018-01-23T14:24:00.807Z\",\"id\":\"7f13df9c-fcbe-4424-914f-2206f106765c\",\"submitted\":1,\"correct\":true}],\"user_submitted\":true,\"id\":\"95194331-ac43-454e-83de-ea8913067055\",\"user_result\":\"correct\"}],\"attempt\":1,\"id\":\"5b28a462-7a3b-42e0-b508-09f3906d1703\",\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}},\"keen_created_at\":\"1516717442.735266\",\"certification\":\"false\",\"keen_id\":\"5a6745820eb8ab00016be1f1\",\"exam_name\":\"Normal Forms and All That Jazz Master Class\"}')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data contained in the first row for understanding the data to unwrap later \n",
    "raw_data.select('value').take(1)[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracted data with map function and json.loads, convert the data to dataframe\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "extracted_data = raw_assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show Schema of the extracted data\n",
    "extracted_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run queries with Spark to answer business questions\n",
    "\n",
    "####  Question 1: How many assesstments are in the dataset?\n",
    "\n",
    "* Answer: 3,280 assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: How many people took *Learning Git*?\n",
    "\n",
    "* Answer: 394 people took *Learning Git*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a temp table name assessments for running spart.sql queries\n",
    "extracted_data.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|   exam_name|number_of_taken|\n",
      "+------------+---------------+\n",
      "|Learning Git|            394|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * \\\n",
    "    from (select exam_name, count(keen_id) as number_of_taken \\\n",
    "    from assessments group by exam_name) \\\n",
    "    where exam_name = 'Learning Git'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: What are the least common courses taken? \n",
    "\n",
    "* Answer:\n",
    "    - Learning to Visualize Data with D3.js\n",
    "    - Nulls, Three-valued Logic and Missing Information\n",
    "    - Native Web Apps for Android\n",
    "    - Operating Red Hat Enterprise Linux Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+---------------+\n",
      "|exam_name                                        |number_of_taken|\n",
      "+-------------------------------------------------+---------------+\n",
      "|Learning to Visualize Data with D3.js            |1              |\n",
      "|Nulls, Three-valued Logic and Missing Information|1              |\n",
      "|Native Web Apps for Android                      |1              |\n",
      "|Operating Red Hat Enterprise Linux Servers       |1              |\n",
      "+-------------------------------------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with t1 as \\\n",
    "    (select exam_name, count(keen_id) as number_of_taken \\\n",
    "    from assessments group by exam_name),\\\n",
    "    t2 as \\\n",
    "    (select min(number_of_taken) as min_taken\\\n",
    "    from t1) \\\n",
    "    select t1.exam_name, t1.number_of_taken \\\n",
    "    from t1 inner join t2 on t1.number_of_taken = t2.min_taken\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4. What are the most common courses taken? \n",
    "\n",
    "* Answer:\n",
    "    - Learning Git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|exam_name   |number_of_taken|\n",
      "+------------+---------------+\n",
      "|Learning Git|394            |\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"with t1 as \\\n",
    "    (select exam_name, count(keen_id) as number_of_taken \\\n",
    "    from assessments group by exam_name),\\\n",
    "    t2 as \\\n",
    "    (select max(number_of_taken) as max_taken\\\n",
    "    from t1) \\\n",
    "    select t1.exam_name, t1.number_of_taken \\\n",
    "    from t1 inner join t2 on t1.number_of_taken = t2.max_taken\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Observation:**\n",
    "    - Could not access the data in the `sequences` column with the map lambda function above. Expected that the keys and values in the nested data do not always exist in all rows and therefore create errors. A customized function will be needed in order to extract keys and values in the `sequences` column.\n",
    "\n",
    "\n",
    "## III. Land the transformed messages in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet file and land in Hadoop /tmp/ directory\n",
    "extracted_data.write.parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check what data are currently in Hadoop after landing the parquet file:\n",
    "\n",
    "    Note: running the following command line in a different terminal window\n",
    "    \n",
    "    - Code summary: list all files in hadoop /tmp/ directory\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "* Further check data in the extracted_assessments folder\n",
    "\n",
    "    - Code summary: list all files in hadoop /tmp/extracted_assessments directory with human readable\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/extracted_assessments\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Spark to read back the parquet file from Hadoop to check if the data was written as expected \n",
    "df = spark.read.parquet(\"/tmp/extracted_assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Further extract nested data & answer more business questions\n",
    "\n",
    "### 1. Extract nested data with custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build customize function to access data in the nested sequences column. \n",
    "# Example of what data included in the key \"counts\" in \"sequences\" as follow:\n",
    "#\"counts\":{\"incomplete\":1,\"submitted\":4,\"incorrect\":1,\"all_correct\":false,\"correct\":2,\"total\":4,\"unanswered\":0}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_sequences(row):\n",
    "    exams = json.loads(row.value)\n",
    "    \n",
    "    exams_data = {\"base_exam_id\": exams[\"base_exam_id\"],\n",
    "                  \"certification\": exams[\"certification\"],\n",
    "                  \"exam_name\": exams[\"exam_name\"],\n",
    "                  \"keen_created_at\": exams[\"keen_created_at\"],\n",
    "                  \"keen_id\": exams[\"keen_id\"],\n",
    "                  \"keen_timestamp\": exams[\"keen_timestamp\"],\n",
    "                  \"max_attempts\": exams[\"max_attempts\"],\n",
    "                  \"started_at\": exams[\"started_at\"],\n",
    "                  \"user_exam_id\": exams[\"user_exam_id\"],\n",
    "                  \"question_attempt\": np.NaN,\n",
    "                  \"question_id\": np.NaN,\n",
    "                  \"question_correct\": np.NaN,\n",
    "                  \"question_number\": np.NaN,\n",
    "                  \"score\": np.NaN                  \n",
    "                 }\n",
    "    \n",
    "    # Unwrap sequences column\n",
    "    if \"sequences\" in exams.keys():\n",
    "        if \"attempt\" in exams[\"sequences\"].keys():\n",
    "            exams_data[\"question_attempt\"] = exams[\"sequences\"][\"attempt\"]\n",
    "\n",
    "        if \"id\" in exams[\"sequences\"].keys():\n",
    "            exams_data[\"question_id\"] = exams[\"sequences\"][\"id\"]\n",
    "\n",
    "        if \"questions\" in exams[\"sequences\"].keys():\n",
    "            exams_data[\"questions\"] = exams[\"sequences\"][\"questions\"]                   \n",
    "\n",
    "        if \"counts\" in exams[\"sequences\"].keys():\n",
    "            if \"correct\" in exams[\"sequences\"][\"counts\"].keys():\n",
    "                exams_data[\"question_correct\"] = exams[\"sequences\"][\"counts\"][\"correct\"]\n",
    "                \n",
    "            if \"total\" in exams[\"sequences\"][\"counts\"].keys():\n",
    "                exams_data[\"question_number\"] = exams[\"sequences\"][\"counts\"][\"total\"]\n",
    "\n",
    "        if exams_data[\"question_number\"] > 0:\n",
    "            exams_data[\"score\"] = 100* exams_data[\"question_correct\"] / exams_data[\"question_number\"]\n",
    "       \n",
    "\n",
    "    return Row(**exams_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe after applying customized extracted function\n",
    "new_df = raw_assessments.rdd.map(extract_sequences).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|question_attempt|question_correct|         question_id|question_number|           questions|score|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|               1|               2|5b28a462-7a3b-42e...|              4|[Map(user_incompl...| 50.0|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|               1|               1|5b28a462-7a3b-42e...|              4|[Map(user_incompl...| 25.0|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|               1|               3|b370a3aa-bf9e-4c1...|              4|[Map(user_incompl...| 75.0|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|               1|               2|b370a3aa-bf9e-4c1...|              4|[Map(user_incompl...| 50.0|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|               1|               3|04a192c1-4f5c-4ac...|              4|[Map(user_incompl...| 75.0|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double check how many assessments in the further extracted dataset\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- question_attempt: long (nullable = true)\n",
      " |-- question_correct: long (nullable = true)\n",
      " |-- question_id: string (nullable = true)\n",
      " |-- question_number: long (nullable = true)\n",
      " |-- questions: array (nullable = true)\n",
      " |    |-- element: map (containsNull = true)\n",
      " |    |    |-- key: string\n",
      " |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print new dataframe schema\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a temp table for running spark.sql queries\n",
    "new_df.registerTempTable('unwrap_assessments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run queries with Spark to answer more business questions\n",
    "\n",
    "#### Question 5: How many different exams in the data set?\n",
    "\n",
    "* Answer: 107 exams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_exams|\n",
      "+-----------+\n",
      "|        107|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct base_exam_id) as total_exams \\\n",
    "    from unwrap_assessments\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: What are the top 10 most difficult exams (lowest score) among the common exams (>50 people take them)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+-------------+---------------+\n",
      "|exam_name                                                  |average_score|number_of_taken|\n",
      "+-----------------------------------------------------------+-------------+---------------+\n",
      "|Software Architecture Fundamentals Understanding the Basics|47.9         |109            |\n",
      "|Intermediate Python Programming                            |51.3         |158            |\n",
      "|Learning to Program with R                                 |54.5         |128            |\n",
      "|Beginning C# Programming                                   |55.5         |95             |\n",
      "|Learning Linux System Administration                       |55.5         |59             |\n",
      "|Introduction to Python                                     |56.7         |162            |\n",
      "|Mastering Git                                              |58.8         |77             |\n",
      "|Practical Java Programming                                 |59.4         |53             |\n",
      "|HTML5 The Basics                                           |60.6         |52             |\n",
      "|Learning Apache Maven                                      |60.9         |80             |\n",
      "+-----------------------------------------------------------+-------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, \\\n",
    "    round(avg(score),1) as average_score, \\\n",
    "    count(keen_id) as number_of_taken \\\n",
    "    from unwrap_assessments group by exam_name \\\n",
    "    having number_of_taken >= 50 \\\n",
    "    order by average_score, number_of_taken desc\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7: What are the top 10 easiest exams (highest score)  among the common exams (>50 people take them)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+-------------+---------------+\n",
      "|exam_name                                                     |average_score|number_of_taken|\n",
      "+--------------------------------------------------------------+-------------+---------------+\n",
      "|Introduction to Java 8                                        |87.6         |158            |\n",
      "|Beginning Programming with JavaScript                         |76.6         |79             |\n",
      "|Python Epiphanies                                             |74.2         |51             |\n",
      "|Learning SQL                                                  |73.7         |57             |\n",
      "|Advanced Machine Learning                                     |72.4         |67             |\n",
      "|Learning Eclipse                                              |70.6         |85             |\n",
      "|Introduction to Machine Learning                              |68.7         |119            |\n",
      "|Learning Git                                                  |67.6         |394            |\n",
      "|JavaScript: The Good Parts Master Class with Douglas Crockford|65.5         |58             |\n",
      "|Introduction to Big Data                                      |61.7         |75             |\n",
      "+--------------------------------------------------------------+-------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, \\\n",
    "    round(avg(score),1) as average_score, \\\n",
    "    count(keen_id) as number_of_taken \\\n",
    "    from unwrap_assessments group by exam_name \\\n",
    "    having number_of_taken >= 50 \\\n",
    "    order by average_score desc\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8: What are the top 10 exams having the most questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+------------------+---------------+\n",
      "|exam_name                                   |number_of_question|number_of_taken|\n",
      "+--------------------------------------------+------------------+---------------+\n",
      "|Operating Red Hat Enterprise Linux Servers  |20                |1              |\n",
      "|Great Bash                                  |10                |14             |\n",
      "|Learning Linux System Administration        |8                 |59             |\n",
      "|Being a Better Introvert                    |7                 |10             |\n",
      "|What's New in JavaScript                    |7                 |2              |\n",
      "|Learning to Program with R                  |7                 |128            |\n",
      "|Introduction to Data Science with R         |7                 |43             |\n",
      "|Understanding the Grails 3 Domain Model     |7                 |2              |\n",
      "|Using Web Components                        |6                 |3              |\n",
      "|Introduction to Modern Front-End Development|6                 |6              |\n",
      "+--------------------------------------------+------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, \\\n",
    "    max(question_number) as number_of_question, \\\n",
    "    count(keen_id) as number_of_taken \\\n",
    "    from unwrap_assessments group by exam_name \\\n",
    "    having number_of_question > 0 \\\n",
    "    order by number_of_question desc\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9: Top 10 who take the most exams ranking by average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------------+-------------+\n",
      "|user_exam_id                        |number_of_exams_taken|average_score|\n",
      "+------------------------------------+---------------------+-------------+\n",
      "|a7e6fc04-245f-4e3c-9539-e2aac44c0eb8|3                    |100.0        |\n",
      "|949aa36c-74c7-4fc1-a41f-42386c1beb37|3                    |100.0        |\n",
      "|b7ac6d15-97e1-4e94-a09d-da819024b8cd|3                    |100.0        |\n",
      "|bd96cfbe-1532-4ba2-a504-7e8a437a5065|3                    |100.0        |\n",
      "|d4ab4aeb-1368-4866-bc5e-7eee69fd1608|3                    |100.0        |\n",
      "|fa23b287-0d0a-4683-8d19-38a65b7f57d1|3                    |100.0        |\n",
      "|37cf5b0c-4807-4214-8426-fb1731b57700|3                    |100.0        |\n",
      "|cdc5859d-b332-4fb1-aae4-5cacb52cea5f|3                    |80.0         |\n",
      "|1e325cc1-47a9-4808-8f6b-508b5459ed6d|3                    |75.0         |\n",
      "|66d91177-c436-4ee1-b0b0-daa960e1b2d0|3                    |75.0         |\n",
      "+------------------------------------+---------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select user_exam_id, \\\n",
    "    count(keen_id) as number_of_exams_taken, \\\n",
    "    round(avg(score),1) as average_score \\\n",
    "    from unwrap_assessments group by user_exam_id \\\n",
    "    order by number_of_exams_taken desc, average_score desc\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Land further extracted data in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write data to parquet file and land in Hadoop /tmp/ directory\n",
    "new_df.write.parquet(\"/tmp/futher_unnested_assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check what data are currently in Hadoop after landing the parquet file:\n",
    "\n",
    "    Note: running the following command line in a different terminal window\n",
    "    \n",
    "    - Code explanation: list all files in hadoop /tmp/ directory\n",
    "\n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "```\n",
    "\n",
    "* Further check data in the futher_unnested_assessments folder\n",
    "\n",
    "    - Code explanation: list all files in hadoop /tmp/futher_unnested_assessments directory with human readable\n",
    "    \n",
    "```\n",
    "docker-compose exec cloudera hadoop fs -ls -h /tmp/futher_unnested_assessments\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Spark to read back the parquet file from Hadoop to check if the data was written as expected \n",
    "df2 = spark.read.parquet(\"/tmp/futher_unnested_assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|question_attempt|question_correct|         question_id|question_number|           questions|score|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|               1|               2|5b28a462-7a3b-42e...|              4|[Map(user_incompl...| 50.0|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|               1|               1|5b28a462-7a3b-42e...|              4|[Map(user_incompl...| 25.0|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|               1|               3|b370a3aa-bf9e-4c1...|              4|[Map(user_incompl...| 75.0|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|               1|               2|b370a3aa-bf9e-4c1...|              4|[Map(user_incompl...| 50.0|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|               1|               3|04a192c1-4f5c-4ac...|              4|[Map(user_incompl...| 75.0|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+----------------+----------------+--------------------+---------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Spin down Docker containers\n",
    "\n",
    "* Spin down docker containers from the docker-compose.yml file:\n",
    "\n",
    "```\n",
    "docker-compose down\n",
    "```\n",
    "\n",
    "* Check if the containers are down:\n",
    "\n",
    "```\n",
    "docker-compose ps\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
